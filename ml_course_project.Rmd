---
title: "Machine Learning - Course Project"
author: "Momoko Price"
date: "7/2/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Course Project: Classifying Weight-Lifting Movement with Barbells

In this project, our goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to identify the class of movement they are performing. Here are the 5 classes of movement, as described in the [original research paper](http://groupware.les.inf.puc-rio.br/har): 

A - Correct movement
B - Throwing elbows to the front
C - Lifting only halfway
D - Lowering only halfway
E - Throwing hips to the front


### STEP 0: LOAD REQUIRED LIBRARIES & DATA

```{r load, echo=FALSE}

library(caret)
library(ggplot2)

raw_train <- read.table("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", sep=",", header=T, na.strings=c("NA"," ", ""))
raw_test <- read.table("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", sep=",", header=T, na.strings=c("NA"," ", ""))

```


### Step 1: Clean the Data

First we take a look at the data to see what we're dealing with:

```{r check, echo=TRUE}

dim(raw_train)
str(raw_train)

```


It appears that several columns in the data set have lots of NAs in them. Let's find out how prevalent these NAs — and by extension, whether we should remove them — by calculating the % of NAs for each variable. 

```{r colmeans, echo=TRUE}

colMeans <- summary(colMeans(is.na(raw_train)))
colMeans

```

Variables in this data set appear to either be completely full (NAs = 0) or nearly empty (NAs = 0.9793). Obviously keeping variables that are 98% empty would be problematic, so let's remove them from both the test and training datasets: 

```{r cleandata1, echo=TRUE}

fullVars <- names(which(colMeans(is.na(raw_train)) == 0))
fullVars2 <- names(which(colMeans(is.na(raw_test)) == 0))

training <- raw_train[,fullVars] 
testing <- raw_test[,fullVars2] 

```

Let's also remove non-quantitative variables such as timestamps, usernames, etc., since our model needs to classify an observed movement based on accelerometer measurements alone:

```{r cleandata2, echo=TRUE}

training <- training[,-(1:7)]
testing <- testing[,-(1:7)]

```

We now have reasonably clean data sets to build our model with. 

### STEP 2: Create a Validation Set (Train and Test Sets)

The n of our training set is quite large (~19,600), so let's break off some data to use as a validation set for checking our model and estimating our out-of-sample error before using it to classify observations from our testing set: 

```{r splitdata }

set.seed(123)
splitIndex <- createDataPartition(training[,"classe"], p=.75, list=F, times=1)
train <- training[splitIndex,]
test <- training[-splitIndex,]
```

### STEP 3: Parametric Model: Linear Discriminant Analysis

Our goal for this project is to predict what type of movement is being observed based on a range of quantitative predictors. In other words, we need to build a classification model as opposed to a regression model. 

For classification problems with a multi-class outcome (as opposed to a binary yes/no outcome), 2 standard options to try are Linear or Quadratic Discrimant Analysis and K-Nearest Neighbours. 

First, let's try LDA, out of the box, using the default cross-validation settings on the training data and centering & scaling the data using the pre-processing function: 

```{r lda, echo=TRUE}

set.seed(213)
lda_fit <- train(classe ~ .,
                 data = train,
                 method = "lda", 
                 preProcess=c("center", "scale"), 
                 trControl = trainControl(method = "cv"))

lda_fit
```

Our model gives in-sample accuracy of ~0.7. Decent, but not great, especially considering in-sample accuracy will likely be overly optimistic compared to out-of-sample accuracy. 

LDA assumes Gaussian distributions of predictor variables and common co-variance, and we don't know this is the case, which may be why the accuracy is subpar. Let's try a non-parametric classifier like K-Nearest-Neighbours and see what we get. 

### STEP 4: Non-Parametric Model: K-Nearest Neighbours

KNN is an extremely low-bias/highly-flexible option, but may result in over-fitting the data. In addition, while it can be highly accurate in predictions, it's generally pretty hard to get any interpretation of relationships from it. But we're only interested in getting accurate predictions, so let's give it a shot. 

Note that we also use principal components analysis (PCR) to reduce the dimensionality of our predictors in this model and hopefully get rid of some unnecessary noise. PCA pre-processing scales and centers the data as well. 

```{r knn, echo=TRUE}

set.seed(435)
knn_fit <- train(classe ~ .,
                 data = train,
                 method = "knn", 
                 preProcess=c("pca"), 
                 trControl = trainControl(method = "cv"))
knn_fit
```

We now get in-sample accuracy estimates of ~0.96 with k = 5, which is a HUGE improvement over LDA. Let's further validate our model by running predictions against the validation set we created.  

```{r lda_classes }

fit_classes1 <- predict(knn_fit, newdata = test)

```

To calculate overall prediction accuracy and the out-of-sample error rate, we generate a Confusion Matrix:

```{r lda_conf, echo=TRUE}

Conf_Matrix1 <- confusionMatrix(data = fit_classes1, test$classe)
Conf_Matrix1

```

Our out-of-sample error rate is only ~4% (i.e. 1-Accuracy). That should be good enough to pass our prediction test. That being said, this model does not give us much with which to INTERPRET the relationships between the predictors and the outcome. We can interpret accurately, but we have no idea how we're doing it!

### STEP 6 (BONUS): Generalized Boosted Regression Models

GBM models are great because they combine high predictive accuracy with highly interpretable results. The plot() function allows you to actually visually see the importance of certain variables over others. The one disadvantage is that a GBM Model can be computationally expensive and take time to run. 

```{r gbm, echo=TRUE}
set.seed(555)
gbm_fit <- train(classe ~ .,
  data = train,
  method = "gbm",
  trControl = trainControl(method = "cv"),
  verbose = F
)
```

The in-sample accuracy of this model is only marginally better than KNN (0.961 vs 0.956). Let's test it with our validation set: 

```{r gbm_conf, echo=TRUE}
fit_classes2 <- predict(gbm_fit, newdata = test)
Conf_Matrix2 <- confusionMatrix(fit_classes2, test$classe)
Conf_Matrix2
```

Again, our out-of-sample accuracy is only marginally better than what we got with KNN (0.9613 vs 0.9592). 

But now let's dig into our gbm model a bit to understand it: 

```{r tail_var_im, echo=TRUE}
var_importance <- summary(gbm_fit)
tail(var_importance, 10)
```

Wow, there are a lot of predictors we've included in our model that have ZERO influence on its predictive power. Now if we look at the most influential variables: 
```{r head_var_im, echo=TRUE}
head(var_importance, 10)
```

We can see that just the top few variables — namely roll_belt, pitch_forearm, yaw_belt, and a few others, have by far the biggest influence on the model. If we wanted to in the future, we could try creating simpler multiple regression models to quantify the mean difference in these predictor variables between classes and get a more granular understanding of what constitutes "good" and "bad" weight-lifting form. 



